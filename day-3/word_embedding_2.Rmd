---
title: "R Text Analysis - day 3 (word embedding sup)"
theme: readable
output:
  html_document: 
    number_sections: yes
    toc: yes
    toc_float: yes
    fig_width: 12
    fig_height: 7
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

https://cbail.github.io/textasdata/word2vec/rmarkdown/word2vec.html
https://programminghistorian.org/en/lessons/basic-text-processing-in-r
https://smltar.com/tokenization.html



```{r}
library(tidyverse)
library(tidytext)
library(widyr)
library(quanteda)
library(tokenizers)
library(irlba)

options(scipen = 999)



```


```{r}

smile_df <- read_csv("data/smile-annotations-final.csv")

```


```{r}

at_pattern <- "@(\\w+)"

url_pattern <- "https?:\\/\\/.*[\r\n]*"

hashtag_pattern <- "(?:^|\\s)[ï¼ƒ#]{1}(\\w+)"

digit_pattern <- '\\d+'

punctuation_pattern <-  "[[:punct:]]" 



```

```{r}


custom_stop_words <- 
  bind_rows(tibble(word = c("amp", "le"),  
                   lexicon = c("custom")), 
                   stop_words)


smile <- smile_df %>% 
  select(-sentiment_category) %>% 
  mutate(tweet_text = str_remove_all(tweet_text, at_pattern),
         tweet_text = str_remove_all(tweet_text, url_pattern),
         tweet_text = str_remove_all(tweet_text, hashtag_pattern),
         tweet_text = str_remove_all(tweet_text, digit_pattern),
         tweet_text = str_remove_all(tweet_text, punctuation_pattern)
         ) %>% 
  anti_join(custom_stop_words, by = c("tweet_text" = "word"))

smile[,2]

```


```{r}

#create context window with length 8
tidy_skipgrams <- smile %>%
    unnest_tokens(ngram, tweet_text, token = "ngrams", n = 8) %>%
    mutate(ngram_id = row_number()) %>% 
    tidyr::unite(skipgram_id, id, ngram_id) %>%
    unnest_tokens(word, ngram) %>% 
    anti_join(custom_stop_words) %>% 
  na.omit()

  
tidy_skipgrams[,2]
```

```{r}

#calculate unigram probabilities (used to normalize skipgram probabilities later)
unigram_probs <- smile %>%
    unnest_tokens(word, tweet_text) %>%
   anti_join(custom_stop_words) %>% 
    count(word, sort = TRUE) %>%
    mutate(p = n / sum(n))

unigram_probs
```


```{r, warning=FALSE}

#calculate probabilities
skipgram_probs <- tidy_skipgrams %>%
    pairwise_count(word, skipgram_id, diag = TRUE, sort = TRUE) %>%
  na.omit() %>% 
  filter(item1 != "amp") %>% 
    mutate(p = n / sum(n))

skipgram_probs
```


```{r}

#normalize probabilities
normalized_prob <- skipgram_probs %>%
    filter(n > 20) %>%
    rename(word1 = item1, word2 = item2) %>%
    left_join(unigram_probs %>%
                  select(word1 = word, p1 = p),
              by = "word1") %>%
    left_join(unigram_probs %>%
                  select(word2 = word, p2 = p),
              by = "word2") %>%
    mutate(p_together = p / p1 / p2)

normalized_prob 


```


```{r}

normalized_prob %>% 
    filter(word1 == "pollock") %>%
    arrange(-p_together)


```


```{r}

pmi_matrix <- normalized_prob %>%
    mutate(pmi = log10(p_together)) %>%
    cast_sparse(word1, word2, pmi)

library(irlba)




```



```{r}

#remove missing data
pmi_matrix@x[is.na(pmi_matrix@x)] <- 0
#run SVD
pmi_svd <- irlba(pmi_matrix, 256, maxit = 500)
#next we output the word vectors:
word_vectors <- pmi_svd$u
rownames(word_vectors) <- rownames(pmi_matrix)

```



```{r}

library(broom)

search_synonyms <- function(word_vectors, selected_vector) {

    similarities <- word_vectors %*% selected_vector %>%
        tidy() %>%
        as_tibble() %>%
        rename(token = .rownames,
               similarity = unrowname.x.)

    similarities %>%
        arrange(-similarity)    
}



```



```{r}

pres_synonym <- search_synonyms(word_vectors,word_vectors["pollock",])
pres_synonym
```



```{r}

pmi_svd <- irlba(pmi_matrix, 2, maxit = 500)

#next we output the word vectors:
word_vectors <- pmi_svd$u
rownames(word_vectors) <- rownames(pmi_matrix)

#grab 100 words
forplot<-as.data.frame(word_vectors[200:300,])
forplot$word<-rownames(forplot)

#now plot
library(ggplot2)
ggplot(forplot, aes(x=V1, y=V2, label=word))+
  geom_text(aes(label=word),hjust=0, vjust=0, color="blue")+
  theme_minimal()+
  xlab("First Dimension Created by SVD")+
  ylab("Second Dimension Created by SVD")

```


```{r}



```


```{r}



```


```{r}



```


```{r}



```


```{r}



```


```{r}



```

```{r}



```


```{r}



```